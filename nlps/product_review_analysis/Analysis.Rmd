---
title: "Product Review Sentiment Analysis"
output: rmarkdown::github_document
---
```{r}
library(KoNLP)
library(rJava)
library(dplyr)
library(readr)
library(ggplot2)
library(tidyr)
library(tidytext)
library(stringr)
library(RWeka)
library(tm)
library(SnowballC)

```

## 목차
0. 프로젝트 목적
1. 프로젝트 개요
2. 내용
3. 결과
4. 분석
5. 개선사항

* 여러개의 제품 리뷰 중에서
* 가장 긍정적인 리뷰와 부정적인 리뷰를 추출하고
* 각각에서 키워드 찾기

# 1. 프로젝트 개요
* 제품 리뷰는 크롤링을 이용해서 csv파일로 수집
* 감성 분석은 감성분석 사전을 이용해서 점수화
* 점수가 높은 집단과 낮은 집단을 추출
* 강점과 약점에 대한 키워드 추출은 tfidf를 이용


-> group_by 감성 -> 긍정/부정적인 리뷰에서 자주 등장한 단어 추출
-> 강점 / 약점 분석 

# 2-1. 감성분석
```{r}
# 기준치 설정 
score.neg = -4
score.pos = 5


my.text.location = "C:/Users/user/Desktop/CampusProject/SentimentAnalysis/ReviewData/apple-airpods.csv"

### csv 불러오기
reviews<-read_csv(my.text.location)

### 열 이름 주기
colnames(reviews)<-c("review.id", "content")

### 단어 토큰으로 쪼개기 
reviews %>% 
  unnest_tokens(word, content) ->
  reviews.word

### 5. 감성사전으로 점수 매기기
inner_join(reviews.word, get_sentiments("bing"), by="word")


# 6.score column 추가 
reviews.word %>% 
  inner_join(get_sentiments("bing"), by="word") %>% 
  count(word, review.id, sentiment) %>% 
  spread(sentiment, n, fill=0) %>% 
  arrange(review.id) ->
  reviews.result

print(reviews.result)
```


```{r}

# 7.계산
reviews.result %>% 
  group_by(review.id) %>% 
  summarise(pos.sum = sum(positive),
            neg.sum = sum(negative),
            score = pos.sum-neg.sum) ->
  reviews.result2

# reviews<-reviews.origin
# View(reviews)
# print(reviews)
# class(reviews)
# View(reviews.result2)
# print(reviews.result)
# class(reviews.result)

#View(left_join(reviews, reviews.result2, by="review.id", all=T)
reviews.result3<-left_join(reviews, reviews.result2, by="review.id", all=T)
length(reviews.result3$score)

#부정적인 리뷰 
target<-(reviews.result3$score<=score.neg)
target<-replace_na(target, FALSE)
target.neg<-which(target)


#긍정적인 리뷰
target<-(reviews.result3$score>=score.pos)
target<-replace_na(target, FALSE)
target.pos<-which(target)

print(target.pos)

View(reviews.result3)
```

## 2-2. 키워드 추출을 위한 전처리

1. 공백처리
2. 특수문자 처리
3. 숫자 처리
4. 불용어 처리
5. n-gram

```{r}
mycorpus<-VCorpus(VectorSource(reviews$content))
DocumentTermMatrix(mycorpus)

temp2<-mycorpus

for(i in 1:length(mycorpus)){
  # 1. 공백, 숫자, 기호 전처리
  temp2[[i]]$content<-str_replace_all(temp2[[i]]$content,"[[:space:]]{1,}", " ")
  temp2[[i]]$content<-str_replace_all(temp2[[i]]$content,"[[:digit:]]{1,}","")
  temp2[[i]]$content<-str_replace_all(temp2[[i]]$content,"[[:punct:]]{1,}","")
  
}

# 2. 불용어 사전 이용해서 불용어 제거
tm_map(temp2, FUN = removeWords, words = stopwords("en"))->temp2


# 3. wordstem으로 어근 변환 # library(SnowballC)
for(i in 1:length(mycorpus)){
  temp2[[i]]$content<-wordStem(temp2[[i]]$content)
}

# 4. 단어 추출 
for(i in 1:length(mycorpus)){
  temp2[[i]]$content <- paste(unlist(str_extract_all(temp2[[i]]$content, boundary("word"))), collapse = " ")
}

# length(mycorpus)
# temp2[[12]]$content
# length(temp2[[1]]$content)
```

## 2-3. tf-idf이용 키워드 추출(시각화)
```{r}
# 5. TDM 생성
TermDocumentMatrix(temp2)->tdm.a
tdm.a

#View(inspect(tdm.a[1:10,1:20]))
# tfidf 생성
weightTfIdf(tdm.a)->tp2

#View(tp2[,n])

print(paste("target.pos :", paste(target.pos, collapse = " "), collapse = " "))
print(paste("target.neg :", paste(target.neg, collapse = " "), collapse = " "))

# 부정 키워드 상위 n개 출력

n.keyword=5
print("---------------------negative---------------")
for(neg in target.neg){
  neg.top5<-tp2[,neg]$i[order(tp2[,neg]$v, decreasing = T)]
  neg.top5<-head(neg.top5,n.keyword)
  print(tp2[,neg]$dimnames$Terms[neg.top5])
  #print(reviews.result3$content[neg])
}


# 긍정 키워드 상위 n개 출력
print("------------------positive--------------------")
for(pos in target.pos){
  pos.top5<-tp2[,pos]$i[order(tp2[,pos]$v, decreasing = T)]
  pos.top5<-head(pos.top5,n.keyword)
  print(tp2[,pos]$dimnames$Terms[pos.top5])
  #print(reviews.result3$content[pos])
}



```

## 2-4. 

```{r}
# (tp1$v)
```

## 3. 결과(그래프)
```{r}

```

##4. 개선사항
* 키워드 추출에서 의미없는 단어
* 리뷰에서 주요 문장. 


































* source: https://www.productreview.com.au/
