---
title: "190524 R 텍스트 전처리"
output: rmarkdown::github_document
---
```{r}
library(stringr)
library(tm)
library(dplyr)
library(ggplot2)
library(SnowballC)
```

### 공백 처리하기
```{r}
#각각 " ", "  ", "\t"으로 구분된다.
mytext<-c("software environment",
  "software  environment",
  "software\tenvironment")
mytext

mt<-str_split(mytext," ")
mt

```

```{r}
# " ", "  ", "\t" -> " "
str_replace_all(mytext,"[[:space:]]{1,}", " ")->mt2

sapply(mt2, length)
sapply(mt2, str_length)
```

### extract_all에 boundary 옵션으로 특수문자 처리
```{r}

mytext<-"The 45th President of the United States, Donald Trump, states that he knows how to play trump with the former president"

str_split(mytext, " ") 
#,가 안없어짐

# 고급함수
str_extract_all(mytext, boundary("word"))
#,가 없어짐

myword<-unlist(str_extract_all(mytext, boundary("word")))
myword

myword<-str_replace(myword,"Trump", "Trump_unique_")
myword<-str_replace(myword,"States", "States_unique_")
tolower(myword)

```


### 숫자 처리하기
```{r}

# 구분 될 필요가 있는 숫자

mytext<-c("He is one of statisticians agreeing that R is the No. 1 statistical software.","He is one of statisticians agreeing that R is the No. one statistical software.")
str_split(mytext," ")

str_replace_all(mytext, "[[:digit:]]{1,}[[:space:]]{1,}", "") -> mytext2
str_split(mytext2, " ")->mytext2
mytext2
str_c(mytext2[[1]], collapse = " ")
str_c(mytext2[[2]], collapse = " ")
```


```{r}

# 숫자 처리 
mytext3<-str_replace_all(mytext, "[[:digit:]]{1,}", "_number_")
mytext3
mytext3<-str_split(mytext3, " ")
mytext3

```

### 특수문자, 불용어 처리하기 
```{r}

# 구분 될 필요가 있는 특수문자 
mytext<-"Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet." # Baek et al = Baek 외에 여러명

#str_split(mytext, "\\.")

# "성씨 다음 et al.이 오고, 이어서(년도)형식
# => "_reference_"로 일괄 치환
# 하이픈 기호 

# 1. 하이픈
mytext2<-str_replace_all(mytext, "-", " ")
mytext2<-str_replace_all(mytext2,"[[:upper:]]{1}[[:alpha:]]{1,}[[:space:]]{1}(et al\\.)[[:space:]]{1}\\([[:digit:]]{4}\\)","_reference_")
mytext2

# 2. . 공백 제거
mytext2<-str_replace_all(mytext2,"\\.[[:space:]]{0,}","")
mytext2

# 3. 불용어 직접 등록, 제거
mystopwords<- "(\\ba )|(\\ban )|(\\bthe )"

mytext<-c("she is an actor", "She is the actor")
str_replace_all(mytext, mystopwords,"")

# 4. 정의되어있는 불용어 사용하기
# stopwords("en") # 짧은 불용어
# stopwords("SMART") # 긴 불용어

# 어근 동일화 처리
# 시제 고려 -> 동일화
# ~s ~es -> 동일화


```


### 불용어 처리 함수
```{r}
mystemmer.func<-function(mytext){
  mytext<-str_replace_all(mytext, "(\\bam )|(\\bare )|(\\bas )|(\\bwas )|(\\bwere )|(\\bbe)", "be ")
print(mytext)
}

test<-c("i am a boy. You are a boy. He mitght be a boy")

mystemmer.func(test)


```


## n-gram
```{r}
# n-gram : 1-gram(의미없음)
#          2-gram(bi-gram), 3-gram(tri-gram)
# n번 연이어 등장하는 단어들의 연결
# 등장배경 : 문장에 대해 이해하기 위해서
# 단어 분리시 문장에 대한 의미가 어려워진다.
# n gram + bayes => 문맥파악

"The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States." -> mytext

# extract로 split보다 깔끔하게 분리 가능하다. 
str_extract_all(mytext, boundary(type="word"))
str_extract_all(mytext, boundary(type="word")) -> myword

# 등장한 단어의 빈도수
table(myword)

# 모든 단어의 갯수
sum(table(myword))

# 서로 다른 단어의 개수
length(table(myword))

# 위처럼 나누면 united의 states와 동사 states 구분이 안된다.
# 따라서 고유명사는 전처리 할 필요가 있다.

str_replace_all(mytext, "\\bUnited States", "United_States")
str_replace_all(mytext, "\\bUnited States", "United_States")->mytext.2gram
str_extract_all(mytext.2gram, boundary(type="word"))


```

#### Corpus 처리
* Corpus 읽기
  text 파일들을 읽어온다 
```{r}
# library(tm)
my.text.location<-"C:/rwork/Data/papers/papers/"
my.text.location

# tm(text mining) 패키지 이용

# txt파일을 모두 합쳐서 Corpus로 만든다.
VCorpus(DirSource(my.text.location))
VCorpus(DirSource(my.text.location))->mypaper
class(unlist(mypaper))

```

* Corpus 정보 확인
```{r}
class(mypaper)
summary(mypaper)
```


* 한개의 txt문서에 대한 정보 확인
```{r}
mypaper[[2]]$meta
```

* 메타데이터에 새로운 값 입력
```{r}
meta(mypaper[[2]], tag='author')<-"G. D. Hong"
mypaper[[2]]$meta
```

* 분석을 위해 리스트를 풀어주기
```{r}
unlist(mypaper)->mypaper
```


* 단어+특수문자+단어 조합을 가진 것 추출
```{r}
myfunc<-function(x){
  str_extract_all(x,"[[:alnum:]]{1,}[[:punct:]]{1,}[[:alnum:]]{1,}")
  # alnum : letters, numbers // punct : .!?,;
  # str_extract_all(데이터, 추출할 문자열이나 정규표현식) 
}

lapply(mypaper, myfunc) -> mypuncts

unlist(mypuncts)

```


* 수치로된 자료를 추출
```{r}
myfunc2<-function(x){
  str_extract_all(x,"[[:digit:]]{1,}")
}

lapply(mypaper, myfunc2) -> mydigits

table(unlist(mydigits))

```

* 대문자로 시작하는 단어 추출(고유명사)
```{r}
myfunc3<-function(charVector){
  str_extract_all(charVector, "\\b[[:upper:]]{1,}[[:alpha:]]{0,}")
}

lapply(mypaper, myfunc3) -> my.uppers2

#table(unlist(my.uppers2))

```

* 어근 추출 함수 / tm_map 
https://thebook.io/006723/ch10/07/02/
https://studymaps.tistory.com/18
```{r}
VCorpus(DirSource(my.text.location))->mypaper
tm_map(mypaper, removeNumbers) -> mycorpus
mycorpus[[1]]$content

removePunctuation("hello......world")
# library(SnowballC)

# 어근 추출 함수 -> 담주 프로젝트에서 쓰면 유용
wordStem(c("learn", "learns", "learning", "learned"))


```
```{r}
print(mypaper[[1]]$content)
print('추출 후 ')
cleaned<-tm_map(mypaper, stemDocument)
cleaned[[1]]$content

```

* 문자열 전처리 코드

```{r}
mycorpus<-tm_map(mypaper,removeNumbers) # mypaper corpus 에서 numbers 제거

mytempfunc<-function(myobject,oldexp,newexp){
  tm_map(myobject, 
         content_transformer(function(x,pattern) gsub(pattern, newexp, x)), # pattern<-oldexp
         oldexp) -> newobject
  #print(newobject)
}


mycorpus <- mytempfunc(mycorpus,"-collar","collar")
mycorpus <- mytempfunc(mycorpus,"\\b((c|C)o-)","co")
mycorpus <- mytempfunc(mycorpus,"\\b((c|C)ross-)","cross")
mycorpus <- mytempfunc(mycorpus,"e\\.g\\.","for example")
mycorpus <- mytempfunc(mycorpus,"i\\.e\\.","that is")
mycorpus <- mytempfunc(mycorpus,"\\'s","")
mycorpus <- mytempfunc(mycorpus,"s’","s")
mycorpus <- mytempfunc(mycorpus,"ICD-","ICD")
mycorpus <- mytempfunc(mycorpus,"\\b((i|I)nter-)","inter")
mycorpus <- mytempfunc(mycorpus,"K-pop","Kpop")
mycorpus <- mytempfunc(mycorpus,"\\b((m|M)eta-)","meta")
mycorpus <- mytempfunc(mycorpus,"\\b((o|O)pt-)","opt")
mycorpus <- mytempfunc(mycorpus,"\\b((p|P)ost-)","post")
mycorpus <- mytempfunc(mycorpus,"-end","end")
mycorpus <- mytempfunc(mycorpus,"\\b((w|W)ithin-)","within")
mycorpus <- mytempfunc(mycorpus,"=","is equal to")
mycorpus <- mytempfunc(mycorpus,"and/or","and or")
mycorpus <- mytempfunc(mycorpus,"his/her","his her")
mycorpus <- mytempfunc(mycorpus,"-"," ")

mycorpus[[1]]$content

# (n = )이 (n is equal to )로 바뀜
```

#### 불필요한 공백 제거
```{r}
tm_map(mycorpus, stripWhitespace)
```

* 대소문자 -> 소문자로 치환
```{r}

mycorpus <- tm_map(mycorpus, content_transformer(tolower))
mycorpus[[2]]$content

```

```{r}

tm_map(mycorpus, removeWords, words = stopwords("SMART"))

mycorpus<-tm_map(mycorpus, stemDocument, language="en")


```


### tf-idf
#### * DTM (Document(행) Term(열) Matrix) or TDM
```{r}
# tf-idf 
mycorpus[[1]]$content
dtm.e<-DocumentTermMatrix(mycorpus)
inspect(dtm.e[5:15,10:20])
# sparsity -> 나온단어/전체단어
# 
```


## 연습문제 
#### 1. 캐글 타이타닉 데이터 중, train.csv파일에 있는 Name 컬럼에서 호칭을 추출한 후, 범주형으로 치환하시오.(가장 많이 등장한 상위 5개의 호칭만 치환할 것)
```{r}
path = "C:/rwork/Data/titanic_train.csv"
ttn<-read.csv(path)
class(ttn$Name)
#str_extract_all(ttn$Name,"[[:space:]]{1}(M)[[:alpha:]]{1,}\\",simplify = T)

ttn$sirName<-str_extract_all(ttn$Name,"(\\,)[[:space:]]{1}[[:alpha:]]{1,}(\\.)[[:space:]]{1}",simplify = T)
ttn$sirName<-str_extract_all(ttn$sirName,"[[:alpha:]]{1,}", simplify = T)
head(ttn[c("Name","sirName")])


```

#### 2. 1번 문제에서 각 호칭별 빈도수를 조사하고, 시각화 하시오.

```{r}
ttn$sirName<-as.factor(ttn$sirName)

table(ttn$sirName) -> sirName.table

sirName.table <- as.data.frame(sirName.table)
sirName.table %>% 
  arrange(desc(Freq)) %>% 
  head(5) -> sirName.table.top5
#View(sirName.table.top5)

ggplot(sirName.table.top5) +
  geom_col(aes(x=Var1, y=Freq))
```

#### 3. 뉴욕타임즈 신문에 실린 기사중 일부를 발췌한 후, 텍스트 처리하시오 (숫자 제거, 대문자 시작단어 추출 등 수업에서 다뤄진 기능 위주로 연습할 것)

```{r}

text<-"For the purposes of press freedoms, what matters is not who counts as a journalist, but whether journalistic activities — whether performed by a “journalist” or anyone else — can be crimes in America. The Trump administration’s move could establish a precedent used to criminalize future acts of national-security journalism, said Jameel Jaffer of the Knight First Amendment Institute at Columbia University. “The charges rely almost entirely on conduct that investigative journalists engage in every day,” he said. “The indictment should be understood as a frontal attack on press freedom.” Mr. Demers left the press briefing without taking questions. And a Justice Department official who stayed behind to answer questions on the condition that he would not be named would not address any about how most of the basic actions the indictment deemed felonies by Mr. Assange differed in a legally meaningful way from ordinary national-security investigative journalism — encouraging sources to provide secret information of news value, obtaining it without the government’s permission and then publishing portions of it."

# 특수문자 제거
str_remove_all(text, "[[:punct:]]{1,}")
str_remove_all(text, "[[:punct:]]{1,}")->text2

# 공백 처리
str_replace_all(text2, "[[:space:]]{2,}", " ")
str_replace_all(text2, "[[:space:]]{2,}", " ")->text3

# 대문자 시작단어 추출
str_extract_all(text3, "[[:upper:]]{1,}[[:alpha:]]{1,}")

# 단어 추출
str_extract_all(text3, boundary(type = "word"),simplify = T)


```

