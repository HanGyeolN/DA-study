rnorm(10)
runif(10)
matrix(rnorm(10),c(2,5))
proc.time()
proc.time()
startTime<-proc.time()
z<-rep(0,10000)
x[1]+y[1]->z[i]
x<-1:10000
y<-10001:20000
startTime<-proc.time()
z<-rep(0,10000)
x[1]+y[1]->z[i]
x<-1:10000
y<-10001:20000
startTime<-proc.time()
z<-rep(0,10000)
x[1]+y[1]->z[1]
endTime<-proc.time()
x<-1:10000
y<-10001:20000
startTime<-proc.time()
z<-rep(0,10000)
x[1]+y[1]->z[1]
endTime<-proc.time()
endTime-startTime
x<-1:10000
y<-10001:20000
startTime<-proc.time()
z<-rep(0,10000)
for (i in 1:10000){
z[i]<-x[i]+y[i]
}
endTime<-proc.time()
endTime-startTime
x<-1:10000
y<-10001:20000
startTime<-proc.time()
z<-rep(0,10000)
for (i in 1:10000){
z[i]<-x[i]+y[i]
}
endTime<-proc.time()
endTime-startTime
a<-c(1,2,3)
b<-c(4,2,1)
a==b
a<-c(1,2,3)
b<-c(4,2,1)
a==b
a<-c(1,2,3)
b<-c(4,2,1)
a==b
all(a==b)
exp(a)
a<-c(0,1,2,3)
b<-c(4,2,1)
a==b
all(a==b)
a<-c(0,1,2,3)
b<-c(4,2,1,0)
a==b
all(a==b)
exp(a)
exp(a)
log(a)
x<-1:5
y<-rep(1,length(x))
x<-1:5
y<-rep(1,length(x))
x+y
x+rep(2,5)
max(x)
x<-50:59
max(x)
which.max(x)
x
x<-matrix(c(10,20,10,20),nrow=2)
x
sum(x[,1])
sum(x[:,1])
sum(x[*,1])
sum(x[,1])
sum(x[1:2,1:2])
sum(x[1:2,1])
x<-matrix(c(10,20,10,20),nrow=2)
sum(x[1:2,1])
set.seed(123)
df<-data.frame(k1=c("x","x","y","y","x"),
k2=c("f","s","f","s","f"),
d1=rnorm(5),
d2=rnorm(5))
df
library(dplyr)
group_by(df, k2)
summarise(group_by(df, k2), myMean = mean(d1))
summarise(group_by(df, k1), myMean = mean(d1))
summarise(group_by(df, k1,k2), myMean = mean(d1))
group_by(df, k1, k2)
group_by(df, k1, k2) %>%
summarise(myMean = mean(d1))
spread(exData, k2, myMean)
library(tidyr)
spread(exData, k2, myMean)
spread(exData01, k2, myMean)
group_by(df, k1, k2) %>%
summarise(myMean = mean(d1)) ->
exData01
spread(exData01, k2, myMean)
spread(exData01, k1, myMean)
group_by(df, k1, k2) %>%
summarise(myMean = mean(d1)) ->
exData01
spread(exData01, k1, myMean)
spread(exData01, k2, myMean)
group_by(df, k1, k2) %>%
summarise(myMean = mean(d1)) ->
exData01
exData01
spread(exData01, k1, myMean)
spread(exData01, k2, myMean)
df1<-data.frame(c('b','b','a','c','a','a','b'),
d1=0:6)
df2<-data.frame(k=c('a','b','d'))
df1
df2
df1<-data.frame(c('b','b','a','c','a','a','b'),
d1=0:6)
df2<-data.frame(k=c('a','b','d'),
d2=0:2)
df1
df2
df1<-data.frame(c('b','b','a','c','a','a','b'),
d1=0:6)
df2<-data.frame(k=c('a','b','d'),
d2=0:2)
df1
df2
merge(df1,df2)
df1<-data.frame(k=c('b','b','a','c','a','a','b'),
d1=0:6)
df2<-data.frame(k=c('a','b','d'),
d2=0:2)
df1
df2
merge(df1,df2)
df1<-data.frame(k=c('b','b','a','c','a','a','b'),
d1=0:6)
df2<-data.frame(k=c('a','b','d'),
d2=0:2)
df1
df2
# 공통 key값을 기준으로 병합
merge(df1,df2)
# -> c,d는 공통값이 없어져서 사라짐
merge(df1,df2, all=T)
# -> 사라지지 않도록 옵션션
df1<-data.frame(k=c('b','b','a','c','a','a','b'),
d1=0:6)
df2<-data.frame(k=c('a','b','d'),
d2=0:2)
df1
df2
# 공통 key값을 기준으로 병합
merge(df1,df2)
# -> c,d는 공통값이 없어져서 사라짐
merge(df1,df2, all=T)
# -> 사라지지 않도록 옵션
merge(df1,df2, all.x=T)
news<-read_csv("C:/Users/user/Desktop/newsScraping/a190527.csv",col_names = T)
library(readr)
news<-read_csv("C:/Users/user/Desktop/newsScraping/a190527.csv",col_names = T)
library(readr)
news<-read_csv("C:/Users/user/Desktop/newsScraping/190527.csv",col_names = T)
cont=list()
cont=paste(news$제목," ")
class(cont)
str_replace_all(cont, "[[:punct:]]", "") -> cont2
library(KoNLP)
library(readr)
library(stringr)
news<-read_csv("C:/Users/user/Desktop/newsScraping/190527.csv",col_names = T)
cont=list()
cont=paste(news$제목," ")
class(cont)
str_replace_all(cont, "[[:punct:]]", "") -> cont2
str_replace_all(cont2, "[[:digit:]]", "") -> cont2
extractNoun(cont2) -> cont3
cont3 <- unlist(cont3)
for(i in 1:length(cont3)){
if(str_length(cont3[[i]]) == 1){
cont3[[i]]=""
}
}
cont3
sort(table(cont3),decreasing = T)
library(KoNLP)
library(readr)
library(stringr)
news<-read_csv("C:/Users/user/Desktop/newsScraping/190527.csv",col_names = T)
cont=list()
cont=paste(news$title," ")
class(cont)
str_replace_all(cont, "[[:punct:]]", "") -> cont2
str_replace_all(cont2, "[[:digit:]]", "") -> cont2
extractNoun(cont2) -> cont3
cont3 <- unlist(cont3)
for(i in 1:length(cont3)){
if(str_length(cont3[[i]]) == 1){
cont3[[i]]=""
}
}
cont3
sort(table(cont3),decreasing = T)
install.packages("tidytext")
library(tidytext)
get_sentiments()
get_sentiments("affin")
get_sentiments("afinn")
summary(get_sentiments("afinn"))
type(z)
get_sentiments("afinn")->z
type(z)
class(z)
str(z)
summary(get_sentiments("afinn"))
table(z$score)
which.max(z$score)
z[[316]]
z[316]
z$word[316]
hist(AFINN)
AFINN<-data.frame(get_sentiments("afinn"))
hist(AFINN)
AFINN<-data.frame(get_sentiments("afinn"))
hist(AFINN$score)
hist(AFINN$word)
imhist(AFINN$score)
hist(AFINN$score)
hist(AFINN$score, xlim = c(-6,6))
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,1200))
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600))
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="E0AADF")
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#E0AADF")
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF")
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=2)
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=10)
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=12)
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=19)
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=20)
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=40)
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=14)
BING<-data.frame(get_sentiments("bing"))
BING
AFINN<-data.frame(get_sentiments("afinn"))
hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=14)
BING<-data.frame(get_sentiments("nrc"))
BING
table(oplex$sentiment)
AFINN<-data.frame(get_sentiments("afinn"))
#hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=14)
oplex<-data.frame(get_sentiments("bing"))
table(oplex$sentiment)
AFINN<-data.frame(get_sentiments("afinn"))
#hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=14)
oplex<-data.frame(get_sentiments("bing"))
table(oplex$sentiment)
omolex<-data.frame(get_sentiments("nrc"))
table(emolex$sentiment)
AFINN<-data.frame(get_sentiments("afinn"))
#hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=14)
oplex<-data.frame(get_sentiments("bing"))
table(oplex$sentiment)
emolex<-data.frame(get_sentiments("nrc"))
table(emolex$sentiment)
emolex$word[emolex$sentiment=="anger"]
AFINN<-data.frame(get_sentiments("afinn"))
#hist(AFINN$score, xlim = c(-6,6), ylim = c(0,600), col="#30AADF", breaks=14)
oplex<-data.frame(get_sentiments("bing"))
table(oplex$sentiment)
emolex<-data.frame(get_sentiments("nrc"))
table(emolex$sentiment)
emolex$word[emolex$sentiment=="anger"]
library(tm)
library(stringr)
library(dplyr)
my.text.location <- "C:/rwork/Data/papers/papers/"
mypaper <- VCorpus(DirSource(my.text.location))
inspect(mypaper)
str(mypaper[[1]])
mypaper[[1]]$content
str(mypaper[[1]])
mypaper[[1]]$meta$author
mypaper[[1]]$meta
mypaper[[1]]
mypaper[[1]][1]
mypaper[[1]][2]
mypaper[[1]][1] == mypaper[[1]]$content
mypaper[[1]][1] == mypaper[[1]]$content
mypaper[[1]][2] == mypaper[[1]]$meta
mypaper[[1]][2] == mypaper[[1]]$meta
mypaper[[1]][2] == unlist(mypaper[[1]]$meta)
mypaper[[1]][2] == mypaper[[1]]$meta
class(mypaper[[1]][2])
class(mypaper[[1]]$meta)
class(mypaper[[1]][2])
class(mypaper[[1]][1])
class(mypaper[[1]])
class(mypaper[[1]][1])
class(as.character(mypaper[[1]][1]))
as.character(mypaper[[1]][1])
str_length(as.character(mypaper[[1]][1]))
str_length((mypaper[[1]][1]))
mytxt<-c(NA,24)
mytxt
length(mypaper)
mytxt<-rep(NA,24)
mytxt
for (i in 1:length(mypaper)){
mytxt[i]<-as.character(mypaper[[i]][1])
}
mytxt
mytxt[24]
library(tidytext)
data_frame(paper.id = 1:length(mypaper),
doc = mytxt)
tibble(paper.id = 1:length(mypaper),
doc = mytxt)
data_frame(paper.id = 1:length(mypaper),
doc = mytxt)
my.df.text %>%
unnest_tokens(word, doc)
my.df.text<-data_frame(paper.id = 1:length(mypaper),
doc = mytxt)
my.df.text %>%
unnest_tokens(word, doc)
#creating dataframe1
pd=data.frame(Name=c("Senthil","Senthil","Sam","Sam"),
Month=c("Jan","Feb","Jan","Feb"),
BS = c(141.2,139.3,135.2,160.1),
BP = c(90,78,80,81))
print(pd)
#creating dataframe2
pd_new=data.frame(Name=c("Senthil","Ramesh","Sam"),Department=c("PSE","Data Analytics","PSE"))
print(pd_new)
left_join(pd, pd_new, by="Name")
left_join(pd, pd_new, by="Name")
print(a<-left_join(pd, pd_new, by="Name"))
print(right_join(pd,pd_new, by="Name"))
print(left_join(pd, pd_new, by="Name"))
print(right_join(pd,pd_new, by="Name"))
print(inner_join(pd,pd_new, by="Name"))
print(left_join(pd, pd_new, by="Name"))
print(right_join(pd,pd_new, by="Name"))
print(inner_join(pd,pd_new, by="Name"))
inspect(my.df.text.word)
my.df.text.word <-
my.df.text %>%
unnest_tokens(word, doc) #doc을 word단위로
inspect(my.df.text.word)
my.text.location <- "C:/rwork/Data/papers/papers/"
mypaper <- VCorpus(DirSource(my.text.location))
#inspect(mypaper)
str(mypaper[[1]]) # structure 확인
mypaper[[1]][1] == mypaper[[1]]$content
class(mypaper[[1]]$meta)
#str_length((mypaper[[1]][1])) = 24
mytxt<-rep(NA,24)
mytxt
for (i in 1:length(mypaper)){
mytxt[i]<-as.character(mypaper[[i]][1])
}
mytxt[24]
my.df.text<-data_frame(paper.id = 1:length(mypaper),
doc = mytxt)
my.df.text.word <-
my.df.text %>%
unnest_tokens(word, doc) #doc을 word단위로
inspect(my.df.text.word)
print(class(my.df.text.word))
print(class(my.df.text.word))
print(get_sentiments("bing"))
print(class(get_sentiments("bing")))
print(class(my.df.text.word))
print(class(get_sentiments("bing")))
inner_join(my.df.text.word, get_sentiments("bing"), by="word")
my.df.text.word %>%
inner_join(get_sentiments("bing"), by="word") %>%
count(word, paper.id, sentiment)
print(class(my.df.text.word))
print(class(get_sentiments("bing")))
print(inner_join(my.df.text.word, get_sentiments("bing"), by="word"))
my.df.text.word %>%
inner_join(get_sentiments("bing"), by="word") %>%
count(word, paper.id, sentiment)
my.df.text.word %>%
inner_join(get_sentiments("bing"), by="word") %>%
count(word, paper.id, sentiment) %>%
spread(sentiment, n)
my.df.text.word %>%
inner_join(get_sentiments("bing"), by="word") %>%
count(word, paper.id, sentiment) %>%
spread(sentiment, n, fill=0)
my.df.text.word %>%
inner_join(get_sentiments("bing"), by="word") %>%
count(word, paper.id, sentiment) %>%
spread(sentiment, n, fill=0) ->
myresult.sa
print(my.df.text.word %>%
inner_join(get_sentiments("bing"), by="word") %>%
count(word, paper.id, sentiment) %>%
spread(sentiment, n, fill=0) ->
myresult.sa)
myresult.sa %>%
group_by(paper.id)
myresult.sa %>%
group_by(paper.id) %>%
summarise(pos.sum = sum(positive),
neg.sum = sum(negative))
myresult.sa %>%
group_by(paper.id) %>%
summarise(pos.sum = sum(positive),
neg.sum = sum(negative),
pos.sent = pos.sum-neg.sum)
mytxt
mytxt[4]
list.file
list.files
list.files(path=my.text.location)
list.files(path=my.text.location, all.files = TRUE)
myfilenames<-list.files(path=my.text.location, all.files = TRUE)
print(myfilenames<-list.files(path=my.text.location, all.files = TRUE))
mytxt[18]
paper.name<-myfilenames[3:26]
paper.name
str_extract_all(paper.name,"[0-9]{1,}")
str_extract_all(paper.name,"[0-9]{1,}", simplify = T)
print(paper.year<-unlist(str_extract_all(paper.name,"[0-9]{1,}")))
mytxt[18]
print(myfilenames<-list.files(path=my.text.location, all.files = TRUE))
paper.name<-myfilenames[3:26]
print(pub.year<-unlist(str_extract_all(paper.name,"[0-9]{1,}")))
print("pub.year",pub.year<-unlist(str_extract_all(paper.name,"[0-9]{1,}")))
print(pub.year<-as.numeric(str_extract_all(paper.name,"[0-9]{1,}")))
unlist
print(myfilenames<-list.files(path=my.text.location, all.files = TRUE))
paper.name<-myfilenames[3:26]
print(pub.year<-unlist(str_extract_all(paper.name,"[0-9]{1,}")))
print(pub.tear.df<-data.frame(paper.id, paper.name, paper.year))
# 3. 논문 제목과 년도 Join하기
paper.id<-1:24
print(pub.tear.df<-data.frame(paper.id, paper.name, paper.year))
# 4.
print(my.df.text.word %>%
inner_join(pub.tear.df, by="paper.id"))
# 4.
print(myresult.sa %>%
inner_join(pub.tear.df, by="paper.id"))
library(tidyr)
read.table("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt")->a
read.table("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt", fileEncoding = "UTF-8")->a
a
read.table("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt", fileEncoding = "UTF-8", sep = "\,")->a
read.table("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt", fileEncoding = "UTF-8")->a
read.table("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt", fileEncoding = "UTF-8", quote = "'")->a
read.table("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt", fileEncoding = "UTF-8", sep=",")->a
read("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt", fileEncoding = "UTF-8", sep=",")
read.csv("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt", fileEncoding = "UTF-8", sep=",")
read_tsv("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt", fileEncoding = "UTF-8", sep=",")
read_tsv("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt")
read_delim("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt")
my.text.location = "C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/"
mypaper <- VCorpus(DirSource(my.text.location))
inspect(mypaper)
mypaper$`hyundai-i30.txt`
mypaper[[1]]$content
a<-mypaper[[1]]$content
class(a)
str_split(a,", ")
str_split(a,"', '")
read_csv("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt")
read_csv("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt")
read_csv("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt",quote = "'")
read_csv("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt",quote = "'",col_types = F)
read("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt")
open("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/hyundai-i30.txt")
open("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")
read_delim("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")
read_delim("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt", delim = ", ")
read_delim("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt", delim = ", ", quote = "\'", col_names = FALSE)
read_delim("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt", delim = ", ", quote = "\"|\'", col_names = FALSE)
open("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")
read_csv2("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")
read_csv("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")
read.csv("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")
read.csv2("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")
read_csv2_chunked("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")
read.csv2
read_lines("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")
read_lines("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")->a
a[1]
a[1,1]
a[1][1]
a[[1]]
a[[1]][1]
str_split(a, "\'\, \'")
str_split(a, "\', \'")
str_split(a, "(\', \')|(\", \")|(\', \")|(\", \'))
str_split(a, "(\', \')|(\", \")|(\', \")|(\", \')")
str_split(a, "(\\', \\')|(\\", \\")|(\\', \\")|(\\", \\')")
read_lines("C:/Users/user/Desktop/CampusProject/Sentiment Analysis/ReviewData/lg-gb-450.txt")->a
a
