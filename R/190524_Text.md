R Notebook
================

``` r
library(stringr)
library(tm)
```

    ## Loading required package: NLP

### 공백 처리하기

``` r
#각각 " ", "  ", "\t"으로 구분된다.
mytext<-c("software environment",
  "software  environment",
  "software\tenvironment")
mytext
```

    ## [1] "software environment"  "software  environment" "software\tenvironment"

``` r
mt<-str_split(mytext," ")
mt
```

    ## [[1]]
    ## [1] "software"    "environment"
    ## 
    ## [[2]]
    ## [1] "software"    ""            "environment"
    ## 
    ## [[3]]
    ## [1] "software\tenvironment"

``` r
# " ", "  ", "\t" -> " "
str_replace_all(mytext,"[[:space:]]{1,}", " ")->mt2

sapply(mt2, length)
```

    ## software environment software environment software environment 
    ##                    1                    1                    1

``` r
sapply(mt2, str_length)
```

    ## software environment software environment software environment 
    ##                   20                   20                   20

### extract\_all에 boundary 옵션으로 특수문자 처리

``` r
mytext<-"The 45th President of the United States, Donald Trump, states that he knows how to play trump with the former president"

str_split(mytext, " ") 
```

    ## [[1]]
    ##  [1] "The"       "45th"      "President" "of"        "the"      
    ##  [6] "United"    "States,"   "Donald"    "Trump,"    "states"   
    ## [11] "that"      "he"        "knows"     "how"       "to"       
    ## [16] "play"      "trump"     "with"      "the"       "former"   
    ## [21] "president"

``` r
#,가 안없어짐

# 고급함수
str_extract_all(mytext, boundary("word"))
```

    ## [[1]]
    ##  [1] "The"       "45th"      "President" "of"        "the"      
    ##  [6] "United"    "States"    "Donald"    "Trump"     "states"   
    ## [11] "that"      "he"        "knows"     "how"       "to"       
    ## [16] "play"      "trump"     "with"      "the"       "former"   
    ## [21] "president"

``` r
#,가 없어짐

myword<-unlist(str_extract_all(mytext, boundary("word")))
myword
```

    ##  [1] "The"       "45th"      "President" "of"        "the"      
    ##  [6] "United"    "States"    "Donald"    "Trump"     "states"   
    ## [11] "that"      "he"        "knows"     "how"       "to"       
    ## [16] "play"      "trump"     "with"      "the"       "former"   
    ## [21] "president"

``` r
myword<-str_replace(myword,"Trump", "Trump_unique_")
myword<-str_replace(myword,"States", "States_unique_")
tolower(myword)
```

    ##  [1] "the"            "45th"           "president"      "of"            
    ##  [5] "the"            "united"         "states_unique_" "donald"        
    ##  [9] "trump_unique_"  "states"         "that"           "he"            
    ## [13] "knows"          "how"            "to"             "play"          
    ## [17] "trump"          "with"           "the"            "former"        
    ## [21] "president"

### 숫자 처리하기

``` r
# 구분 될 필요가 있는 숫자

mytext<-c("He is one of statisticians agreeing that R is the No. 1 statistical software.","He is one of statisticians agreeing that R is the No. one statistical software.")
str_split(mytext," ")
```

    ## [[1]]
    ##  [1] "He"            "is"            "one"           "of"           
    ##  [5] "statisticians" "agreeing"      "that"          "R"            
    ##  [9] "is"            "the"           "No."           "1"            
    ## [13] "statistical"   "software."    
    ## 
    ## [[2]]
    ##  [1] "He"            "is"            "one"           "of"           
    ##  [5] "statisticians" "agreeing"      "that"          "R"            
    ##  [9] "is"            "the"           "No."           "one"          
    ## [13] "statistical"   "software."

``` r
str_replace_all(mytext, "[[:digit:]]{1,}[[:space:]]{1,}", "") -> mytext2
str_split(mytext2, " ")->mytext2
mytext2
```

    ## [[1]]
    ##  [1] "He"            "is"            "one"           "of"           
    ##  [5] "statisticians" "agreeing"      "that"          "R"            
    ##  [9] "is"            "the"           "No."           "statistical"  
    ## [13] "software."    
    ## 
    ## [[2]]
    ##  [1] "He"            "is"            "one"           "of"           
    ##  [5] "statisticians" "agreeing"      "that"          "R"            
    ##  [9] "is"            "the"           "No."           "one"          
    ## [13] "statistical"   "software."

``` r
str_c(mytext2[[1]], collapse = " ")
```

    ## [1] "He is one of statisticians agreeing that R is the No. statistical software."

``` r
str_c(mytext2[[2]], collapse = " ")
```

    ## [1] "He is one of statisticians agreeing that R is the No. one statistical software."

``` r
# 숫자 처리 
mytext3<-str_replace_all(mytext, "[[:digit:]]{1,}", "_number_")
mytext3
```

    ## [1] "He is one of statisticians agreeing that R is the No. _number_ statistical software."
    ## [2] "He is one of statisticians agreeing that R is the No. one statistical software."

``` r
mytext3<-str_split(mytext3, " ")
mytext3
```

    ## [[1]]
    ##  [1] "He"            "is"            "one"           "of"           
    ##  [5] "statisticians" "agreeing"      "that"          "R"            
    ##  [9] "is"            "the"           "No."           "_number_"     
    ## [13] "statistical"   "software."    
    ## 
    ## [[2]]
    ##  [1] "He"            "is"            "one"           "of"           
    ##  [5] "statisticians" "agreeing"      "that"          "R"            
    ##  [9] "is"            "the"           "No."           "one"          
    ## [13] "statistical"   "software."

### 특수문자, 불용어 처리하기

``` r
# 구분 될 필요가 있는 특수문자 
mytext<-"Baek et al. (2014) argued that the state of default-setting is critical for people to protect their own personal privacy on the Internet." # Baek et al = Baek 외에 여러명

#str_split(mytext, "\\.")

# "성씨 다음 et al.이 오고, 이어서(년도)형식
# => "_reference_"로 일괄 치환
# 하이픈 기호 

# 1. 하이픈
mytext2<-str_replace_all(mytext, "-", " ")
mytext2<-str_replace_all(mytext2,"[[:upper:]]{1}[[:alpha:]]{1,}[[:space:]]{1}(et al\\.)[[:space:]]{1}\\([[:digit:]]{4}\\)","_reference_")
mytext2
```

    ## [1] "_reference_ argued that the state of default setting is critical for people to protect their own personal privacy on the Internet."

``` r
# 2. . 공백 제거
mytext2<-str_replace_all(mytext2,"\\.[[:space:]]{0,}","")
mytext2
```

    ## [1] "_reference_ argued that the state of default setting is critical for people to protect their own personal privacy on the Internet"

``` r
# 3. 불용어 직접 등록, 제거
mystopwords<- "(\\ba )|(\\ban )|(\\bthe )"

mytext<-c("she is an actor", "She is the actor")
str_replace_all(mytext, mystopwords,"")
```

    ## [1] "she is actor" "She is actor"

``` r
# 4. 정의되어있는 불용어 사용하기
# stopwords("en") # 짧은 불용어
# stopwords("SMART") # 긴 불용어

# 어근 동일화 처리
# 시제 고려 -> 동일화
# ~s ~es -> 동일화
```

### 불용어 처리 함수

``` r
mystemmer.func<-function(mytext){
  mytext<-str_replace_all(mytext, "(\\bam )|(\\bare )|(\\bas )|(\\bwas )|(\\bwere )|(\\bbe)", "be ")
print(mytext)
}

test<-c("i am a boy. You are a boy. He mitght be a boy")

mystemmer.func(test)
```

    ## [1] "i be a boy. You be a boy. He mitght be  a boy"

## n-gram

``` r
# n-gram : 1-gram(의미없음)
#          2-gram(bi-gram), 3-gram(tri-gram)
# n번 연이어 등장하는 단어들의 연결
# 등장배경 : 문장에 대해 이해하기 위해서
# 단어 분리시 문장에 대한 의미가 어려워진다.
# n gram + bayes => 문맥파악

"The United States comprises fifty states. In the United States, each state has its own laws. However, federal law overrides state law in the United States." -> mytext

# extract로 split보다 깔끔하게 분리 가능하다. 
str_extract_all(mytext, boundary(type="word"))
```

    ## [[1]]
    ##  [1] "The"       "United"    "States"    "comprises" "fifty"    
    ##  [6] "states"    "In"        "the"       "United"    "States"   
    ## [11] "each"      "state"     "has"       "its"       "own"      
    ## [16] "laws"      "However"   "federal"   "law"       "overrides"
    ## [21] "state"     "law"       "in"        "the"       "United"   
    ## [26] "States"

``` r
str_extract_all(mytext, boundary(type="word")) -> myword

# 등장한 단어의 빈도수
table(myword)
```

    ## myword
    ## comprises      each   federal     fifty       has   However        in 
    ##         1         1         1         1         1         1         1 
    ##        In       its       law      laws overrides       own     state 
    ##         1         1         2         1         1         1         2 
    ##    states    States       the       The    United 
    ##         1         3         2         1         3

``` r
# 모든 단어의 갯수
sum(table(myword))
```

    ## [1] 26

``` r
# 서로 다른 단어의 개수
length(table(myword))
```

    ## [1] 19

``` r
# 위처럼 나누면 united의 states와 동사 states 구분이 안된다.
# 따라서 고유명사는 전처리 할 필요가 있다.

str_replace_all(mytext, "\\bUnited States", "United_States")
```

    ## [1] "The United_States comprises fifty states. In the United_States, each state has its own laws. However, federal law overrides state law in the United_States."

``` r
str_replace_all(mytext, "\\bUnited States", "United_States")->mytext.2gram
str_extract_all(mytext.2gram, boundary(type="word"))
```

    ## [[1]]
    ##  [1] "The"           "United_States" "comprises"     "fifty"        
    ##  [5] "states"        "In"            "the"           "United_States"
    ##  [9] "each"          "state"         "has"           "its"          
    ## [13] "own"           "laws"          "However"       "federal"      
    ## [17] "law"           "overrides"     "state"         "law"          
    ## [21] "in"            "the"           "United_States"

#### Corpus 처리

  - Corpus 읽기 text 파일들을 읽어온다

<!-- end list -->

``` r
# library(tm)
my.text.location<-"C:/rwork/Data/papers/papers/"
my.text.location
```

    ## [1] "C:/rwork/Data/papers/papers/"

``` r
# tm(text mining) 패키지 이용

# txt파일을 모두 합쳐서 Corpus로 만든다.
VCorpus(DirSource(my.text.location))
```

    ## <<VCorpus>>
    ## Metadata:  corpus specific: 0, document level (indexed): 0
    ## Content:  documents: 24

``` r
VCorpus(DirSource(my.text.location))->mypaper
class(unlist(mypaper))
```

    ## [1] "character"

  - Corpus 정보 확인

<!-- end list -->

``` r
class(mypaper)
```

    ## [1] "VCorpus" "Corpus"

``` r
summary(mypaper)
```

    ##            Length Class             Mode
    ## p2009a.txt 2      PlainTextDocument list
    ## p2009b.txt 2      PlainTextDocument list
    ## p2010a.txt 2      PlainTextDocument list
    ## p2010b.txt 2      PlainTextDocument list
    ## p2010c.txt 2      PlainTextDocument list
    ## p2011a.txt 2      PlainTextDocument list
    ## p2011b.txt 2      PlainTextDocument list
    ## p2012a.txt 2      PlainTextDocument list
    ## p2012b.txt 2      PlainTextDocument list
    ## p2013a.txt 2      PlainTextDocument list
    ## p2014a.txt 2      PlainTextDocument list
    ## p2014b.txt 2      PlainTextDocument list
    ## p2014c.txt 2      PlainTextDocument list
    ## p2014d.txt 2      PlainTextDocument list
    ## p2014e.txt 2      PlainTextDocument list
    ## p2014f.txt 2      PlainTextDocument list
    ## p2014g.txt 2      PlainTextDocument list
    ## p2014h.txt 2      PlainTextDocument list
    ## p2014i.txt 2      PlainTextDocument list
    ## p2014k.txt 2      PlainTextDocument list
    ## p2015a.txt 2      PlainTextDocument list
    ## p2015b.txt 2      PlainTextDocument list
    ## p2015c.txt 2      PlainTextDocument list
    ## p2015d.txt 2      PlainTextDocument list

  - 한개의 txt문서에 대한 정보 확인

<!-- end list -->

``` r
mypaper[[2]]$meta
```

    ##   author       : character(0)
    ##   datetimestamp: 2019-05-24 05:32:48
    ##   description  : character(0)
    ##   heading      : character(0)
    ##   id           : p2009b.txt
    ##   language     : en
    ##   origin       : character(0)

  - 메타데이터에 새로운 값 입력

<!-- end list -->

``` r
meta(mypaper[[2]], tag='author')<-"G. D. Hong"
mypaper[[2]]$meta
```

    ##   author       : G. D. Hong
    ##   datetimestamp: 2019-05-24 05:32:48
    ##   description  : character(0)
    ##   heading      : character(0)
    ##   id           : p2009b.txt
    ##   language     : en
    ##   origin       : character(0)

  - 분석을 위해 리스트를 풀어주기

<!-- end list -->

``` r
unlist(mypaper)->mypaper
```

  - 단어+특수문자+단어 조합을 가진 것 추출

<!-- end list -->

``` r
myfunc<-function(x){
  str_extract_all(x,"[[:alnum:]]{1,}[[:punct:]]{1,}[[:alnum:]]{1,}")
  # alnum : letters, numbers // punct : .!?,;
  # str_extract_all(데이터, 추출할 문자열이나 정규표현식) 
}

lapply(mypaper, myfunc) -> mypuncts

unlist(mypuncts)
```

    ##               content.content1               content.content2 
    ##                      "face-to"                      "face-to" 
    ##               content.content3 content.meta.datetimestamp.sec 
    ##               "within-subject"             "48.3482279777527" 
    ##                content.meta.id                content.content 
    ##                   "p2009a.txt"                "meta-analysis" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3482279777527"                   "p2009b.txt" 
    ##               content.content1               content.content2 
    ##                      "face-to"                    "follow-up" 
    ##               content.content3               content.content4 
    ##                    "follow-up"                    "follow-up" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3482279777527"                   "p2010a.txt" 
    ##               content.content1               content.content2 
    ##           "inter-disciplinary"                "co-activation" 
    ##               content.content3               content.content4 
    ##                 "co-emergence"                "co-constraint" 
    ##               content.content5 content.meta.datetimestamp.sec 
    ##                 "co-existence"             "48.3482279777527" 
    ##                content.meta.id               content.content1 
    ##                   "p2010b.txt"              "socio-political" 
    ##               content.content2               content.content3 
    ##                "post-material"           "lifestyle-oriented" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3482279777527"                   "p2010c.txt" 
    ##                content.content content.meta.datetimestamp.sec 
    ##                  "cross-party"             "48.3482279777527" 
    ##                content.meta.id               content.content1 
    ##                   "p2011a.txt"                   "open-ended" 
    ##               content.content2               content.content3 
    ##                   "open-ended"                   "open-ended" 
    ##               content.content4 content.meta.datetimestamp.sec 
    ##              "content-analyze"             "48.3482279777527" 
    ##                content.meta.id               content.content1 
    ##                   "p2011b.txt"                      "face-to" 
    ##               content.content2               content.content3 
    ##                      "face-to"                       "and/or" 
    ##               content.content4               content.content5 
    ##                      "face-to"              "over-represents" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3482279777527"                   "p2012a.txt" 
    ##               content.content1               content.content2 
    ##                          "e.g"                       "and/or" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3482279777527"                   "p2012b.txt" 
    ##               content.content1               content.content2 
    ##                      "his/her"                          "e.g" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3482279777527"                   "p2013a.txt" 
    ##               content.content1               content.content2 
    ##                          "i.e"                          "i.e" 
    ##               content.content3               content.content4 
    ##                  "large-scale"           "privacy-protective" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3482279777527"                   "p2014a.txt" 
    ##               content.content1               content.content2 
    ##                  "Gudykunst's"             "culture-specific" 
    ##               content.content3               content.content4 
    ##                     "people's"             "cross-culturally" 
    ##               content.content5               content.content6 
    ##                   "Hofstede's"               "mouth-oriented" 
    ##               content.content7               content.content8 
    ##                 "eye-oriented"                  "data-driven" 
    ##               content.content9 content.meta.datetimestamp.sec 
    ##               "cross-cultural"             "48.3482279777527" 
    ##                content.meta.id               content.content4 
    ##                   "p2014b.txt"                        "ICD-9" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3492290973663"                   "p2014c.txt" 
    ##               content.content1               content.content2 
    ##           "privacy-protective"                    "people’s" 
    ##               content.content3               content.content4 
    ##                   "message’s"                          "e.g" 
    ##               content.content5 content.meta.datetimestamp.sec 
    ##                "policy-making"             "48.3492290973663" 
    ##                content.meta.id               content.content1 
    ##                   "p2014d.txt"             "fearful-avoidant" 
    ##               content.content2               content.content3 
    ##          "dismissive-avoidant"           "anxious-ambivalent" 
    ##               content.content4 content.meta.datetimestamp.sec 
    ##                  "SNS-related"             "48.3492290973663" 
    ##                content.meta.id               content.content1 
    ##                   "p2014e.txt"                   "notice-and" 
    ##               content.content2               content.content3 
    ##                        "ISP's"                       "opt-in" 
    ##               content.content4               content.content5 
    ##                      "opt-out"                     "people's" 
    ##               content.content6               content.content7 
    ##                     "people's"                       "opt-in" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3492290973663"                   "p2014f.txt" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3492290973663"                   "p2014g.txt" 
    ##               content.content1               content.content2 
    ##            "self-presentation"                  "self-esteem" 
    ##               content.content3               content.content4 
    ##            "self-presentation"                  "self-esteem" 
    ##               content.content5 content.meta.datetimestamp.sec 
    ##                  "self-esteem"             "48.3492290973663" 
    ##                content.meta.id               content.content1 
    ##                   "p2014h.txt"              "commitment.?The" 
    ##               content.content2               content.content3 
    ##                  "main-effect"              "commitment.?The" 
    ##               content.content4               content.content5 
    ##             "mediation-effect"            "commitment.?Using" 
    ##               content.content6               content.content7 
    ##             "mediation-effect"          "plausible.?Findings" 
    ##               content.content8 content.meta.datetimestamp.sec 
    ##                  "blue-collar"             "48.3492290973663" 
    ##                content.meta.id               content.content1 
    ##                   "p2014i.txt"                          "i.e" 
    ##               content.content2               content.content3 
    ##                          "i.e"                          "i.e" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3492290973663"                   "p2014k.txt" 
    ##               content.content1               content.content2 
    ##               "cross-cultural"               "cross-cultural" 
    ##               content.content3               content.content4 
    ##                  "large-scale"                        "K-pop" 
    ##               content.content5               content.content6 
    ##                   "Hofstede's"             "five-dimensional" 
    ##               content.content7               content.content8 
    ##                        "K-pop"                "export/import" 
    ##               content.content9              content.content10 
    ##               "cross-cultural"               "cross-cultural" 
    ##              content.content11 content.meta.datetimestamp.sec 
    ##                        "K-pop"             "48.3492290973663" 
    ##                content.meta.id               content.content1 
    ##                   "p2015a.txt"                 "news/opinion" 
    ##               content.content2               content.content3 
    ##                 "news/opinion"                "news/opinions" 
    ##               content.content4               content.content5 
    ##                      "other's"                  "within-bloc" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3492290973663"                   "p2015b.txt" 
    ## content.meta.datetimestamp.sec                content.meta.id 
    ##             "48.3492290973663"                   "p2015c.txt" 
    ##                content.content content.meta.datetimestamp.sec 
    ##               "cross-cultural"             "48.3492290973663" 
    ##                content.meta.id 
    ##                   "p2015d.txt"

  - 수치로된 자료를 추출

<!-- end list -->

``` r
myfunc2<-function(x){
  str_extract_all(x,"[[:digit:]]{1,}")
}

lapply(mypaper, myfunc2) -> mydigits

table(unlist(mydigits))
```

    ## 
    ##             0             1            11           119           143 
    ##            24             9             1            24            24 
    ##          1973             2          2002          2003          2004 
    ##             1             9             1             1             1 
    ##          2007          2008          2009          2010          2011 
    ##             1             2             2             3             2 
    ##          2012          2013          2014          2015          2028 
    ##             4             1            10             4             1 
    ##            24             3            32 3482279777527 3492290973663 
    ##            24             7            24            12            12 
    ##            35             4            48             5           712 
    ##             1            26            24            48             1 
    ##           756            78            82             9 
    ##             1             1             1             1
